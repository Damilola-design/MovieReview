{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd46876",
   "metadata": {},
   "source": [
    "Movie Review Dataset\n",
    "\n",
    "\n",
    "Well start by loading in the IMDB movie review dataset from keras. This dataset contains 25,000 reviews from IMDB where each one is already\n",
    "preprocessed and has a label as either positive or negative. Each review is encoded by integers that represents how common a word is in the\n",
    "entire dataset. For example a word encoded by the integer 3 means that it is the 3rd most common word in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1632719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = 88584 #Number of words in the dataset \n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data (num_words = VOCAB_SIZE)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5331dd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at the review \n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f384fd",
   "metadata": {},
   "source": [
    "More Preprocessing\n",
    "\n",
    "If we have a look at some of our loaded in reviews we'll notice that they are different lengths. This is an issue. We cannot pass different length\n",
    "data into our neural network. Therefore we must make each review the same length. To do this we will follow the procedure below:\n",
    "• if the review is greater than 250 words then trim off the extra words\n",
    "• if the review is less than 250 words add the necessary amount of's to make it equal to 250.\n",
    "Luckily for us keras has a function that can do this for us:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a61ea25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pad_sequences(train_data, MAXLEN)\n",
    "test_data = pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3752354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6572a",
   "metadata": {},
   "source": [
    "Creating the Model\n",
    "\n",
    "\n",
    "Now it's time to create the model. We'll use a word embedding layer as the first layer in our model and add a LSTM layer afterwards that feeds\n",
    "into a dense node to get our predicted sentiment.\n",
    "32 stands for the output dimension of the vectors generated by the embedding layer. We can change this value if we'd like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00f1084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding (VOCAB_SIZE, 32), # every single word has 32 dimention\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e16ed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3fddb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 58s 90ms/step - loss: 0.4129 - acc: 0.8108 - val_loss: 0.2804 - val_acc: 0.8840\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.2384 - acc: 0.9090 - val_loss: 0.2828 - val_acc: 0.8796\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 55s 89ms/step - loss: 0.1848 - acc: 0.9341 - val_loss: 0.2778 - val_acc: 0.8790\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 54s 86ms/step - loss: 0.1531 - acc: 0.9473 - val_loss: 0.2787 - val_acc: 0.8920\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 60s 95ms/step - loss: 0.1308 - acc: 0.9541 - val_loss: 0.2878 - val_acc: 0.8856\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 48s 77ms/step - loss: 0.1134 - acc: 0.9614 - val_loss: 0.3418 - val_acc: 0.8934\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 51s 82ms/step - loss: 0.0976 - acc: 0.9672 - val_loss: 0.3815 - val_acc: 0.8840\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 50s 80ms/step - loss: 0.0888 - acc: 0.9712 - val_loss: 0.3663 - val_acc: 0.8638\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 51s 82ms/step - loss: 0.0770 - acc: 0.9756 - val_loss: 0.3290 - val_acc: 0.8866\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 50s 80ms/step - loss: 0.0717 - acc: 0.9769 - val_loss: 0.4077 - val_acc: 0.8770\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e913209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 17s 21ms/step - loss: 0.4729 - acc: 0.8555\n",
      "[0.47288480401039124, 0.8555200099945068]\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68c4ea",
   "metadata": {},
   "source": [
    "So were scoring somewhere in the mid-high 80's. Not bad for a simple recurrent network.\n",
    "\n",
    "\n",
    "- Making Predictions\n",
    "Now lets use our network to make predictions on our own reviews.\n",
    "since our reviews are encoded well need to convert any review that we write into that form so the network can understand it. To do that well\n",
    "load the encodings from the dataset and use them to encode our own data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90db9f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "word_index = imdb.get_word_index()\n",
    "def encode_text (text) :\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text =\"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a48de1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "# while were at it lets make a decode function\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "def decode_integers(integers):\n",
    "                      PAD = 0\n",
    "                      text = \"\"\n",
    "                      for num in integers:\n",
    "                        if num != PAD:\n",
    "                          text += reverse_word_index[num] + \" \"\n",
    "                      \n",
    "                      return text[:-1]\n",
    "print (decode_integers(encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60d0b145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 203ms/step\n",
      "[0.8535881]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[0.45796007]\n"
     ]
    }
   ],
   "source": [
    "# now time to make a prediction\n",
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1, 250))\n",
    "    pred[0] = encoded_text\n",
    "    result = model.predict(pred)\n",
    "    print(result[0])\n",
    "    \n",
    "    \n",
    "positive_review = \"That movie was so awesome! I really loved it and would watch it again because it was amazingly great\"\n",
    "predict(positive_review)\n",
    "negative_review = \"that movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict (negative_review)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46731643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
